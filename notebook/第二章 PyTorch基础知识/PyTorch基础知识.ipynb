{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "useful-nerve",
   "metadata": {},
   "source": [
    "# 第一部分：张量运算示例\n",
    "\n",
    "这里将演示Tensor的一些基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce17080f-aeca-428f-9613-9d18b8f0f97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # check your GPU is working fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c31ab-b374-4782-acce-c1d40bae6611",
   "metadata": {},
   "source": [
    "### 张量的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "healthy-seventh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(1) tensor(1, dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建tensor，用dtype指定类型。注意类型要匹配\n",
    "a = torch.tensor(1.0, dtype=torch.float) # or FloatTensor(1.0)\n",
    "b = torch.tensor(1, dtype=torch.long) \n",
    "c = torch.tensor(1.0, dtype=torch.int8) # or IntTensor (1)\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca14483-90fe-4662-b0fa-6a471e4ef909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 28 28\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(a.item()), sys.getsizeof(b.item()), sys.getsizeof(c.item())) # in bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c3430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4891, 0.5777],\n",
      "         [0.6134, 0.2894]],\n",
      "\n",
      "        [[0.2896, 0.8937],\n",
      "         [0.7426, 0.0927]],\n",
      "\n",
      "        [[0.5749, 0.9190],\n",
      "         [0.2778, 0.0795]]])\n",
      "torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(3, 2, 2) \n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26ff1ba9-034f-4f20-925a-83b1d6001243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9640, 0.2648, 0.1399],\n",
      "        [0.9136, 0.8699, 0.3784]]) \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      " tensor([0, 2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# 常见的构造Tensor的函数\n",
    "k = torch.rand(2, 3) # 0-1 distribution \n",
    "l = torch.ones(2, 3) \n",
    "m = torch.zeros(2, 3)\n",
    "n = torch.arange(0, 10, 2)\n",
    "print(k, '\\n', l, '\\n', m, '\\n', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278bfe9",
   "metadata": {},
   "source": [
    "### 张量的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ff1ac16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4],\n",
       "        [6, 8]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],[3,4]]) \n",
    "x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16094638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53c53611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4],\n",
       "        [6, 8]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.add_(x) # in-place addition\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96ef9acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7, 10],\n",
      "        [15, 22]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],[3,4]]) \n",
    "print(x @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5464887d-242f-4bc7-a06c-bbbcfbec118d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3],\n",
       "        [2, 4]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T \n",
    "# UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb4ee4",
   "metadata": {},
   "source": [
    "### 索引操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f56174a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],[3,4]]) \n",
    "\n",
    "# 取第二列\n",
    "print(x[:, 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6f6db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "torch.Size([2, 2, 2])\n",
      "tensor([3, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]]) \n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "print(x[:, 1, 0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8342351-98bb-4fcf-894d-c2334b944180",
   "metadata": {},
   "source": [
    "想象一个立方体，高度为第一维度，横向为第二维度，纵向为第三维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28b2ac97-4c0d-4c98-b183-ac38843b6248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) \n",
      " <class 'torch.Tensor'> \n",
      " 7 \n",
      " <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print (x[1,1,0],'\\n', type(x[1,1,0]), '\\n', x[1,1,0].item(), '\\n', type(x[1,1,0].item()) ) # 使用 .item() 来获得这个 value，而不获得其他性质"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d812e-be13-48c3-affc-2808678bbaaa",
   "metadata": {},
   "source": [
    "### pytorch tensor 与 numpy ndarray 之间的相互转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "welsh-blank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "<class 'torch.Tensor'>\n",
      "80\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "<class 'numpy.ndarray'>\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "g = np.array([[1,2,3],[4,5,6]])\n",
    "h = torch.tensor(g) # numpy -> torch\n",
    "print(h)\n",
    "\n",
    "i = torch.from_numpy(g) # numpy -> torch\n",
    "print(i)\n",
    "print(type(i))\n",
    "print(sys.getsizeof(i))\n",
    "\n",
    "j = h.numpy() # torch -> numpy\n",
    "print(j)\n",
    "print(type(j))\n",
    "print(sys.getsizeof(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6370e",
   "metadata": {},
   "source": [
    "注意：torch.tensor构造函数创建得到的张量和原数据是不共享内存的，张量对应的变量是独立变量。  \n",
    "而torch.from_numpy()和torch.as_tensor()从numpy array创建得到的张量和原数据是共享内存的，张量对应的变量不是独立变量，修改numpy array会导致对应tensor的改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "562911ea-6cbc-4833-9cf2-525528c96b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[0,0] =0\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05fb5834-1a17-4caa-bbf1-c73904176bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42254c58-13f7-4474-a89b-acef55b18cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "characteristic-lighting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 查看tensor的维度信息（两种方式）\n",
    "print(h.shape)\n",
    "print(h.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46358241-418b-4b2d-b1b9-e6a8fefa74ef",
   "metadata": {},
   "source": [
    "### 改变tensor形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "removed-lawrence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]]) \n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "# 改变tensor形状：view\n",
    "y = x.view((-1,2))\n",
    "print(y.shape)\n",
    "\n",
    "z = x.view(-1)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a23dc9-b6d5-4149-a2fc-38d496293eb9",
   "metadata": {},
   "source": [
    "注: torch.view() 返回的新tensor与源tensor共享内存(其实是同一个tensor)，更改其中的一个，另外一个也会跟着改变。(顾名思义，view()仅仅是改变了对这个张量的观察角度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9bbba97b-7608-4884-97e2-edf78bee1a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[99,  2],\n",
       "         [ 3,  4],\n",
       "         [ 5,  6],\n",
       "         [ 7,  8]]),\n",
       " tensor([99,  2,  3,  4,  5,  6,  7,  8]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0,0] = 99\n",
    "y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b01b3649-8a29-4a91-9756-03efcf442588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hex(id(x)), hex(id(y)), hex(id(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9381a4-41cd-4498-99ec-042c31a749f7",
   "metadata": {},
   "source": [
    "上面我们说过torch.view()会改变原始张量，但是很多情况下，我们希望原始张量和变换后的张量互相不影响。为为了使创建的张量和原始张量不共享内存，我们需要使用第二种方法torch.reshape()， 同样可以改变张量的形状，但是此函数并不能保证返回的是其拷贝值，所以官方不推荐使用。推荐的方法是我们先用 clone() 创造一个张量副本然后再使用 torch.view()进行函数维度变换 。\n",
    "\n",
    "注：使用 clone() 还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 Tensor 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13def931-4f47-456c-8460-706947b55317",
   "metadata": {},
   "source": [
    "\n",
    "    Tensor.view() works only on contiguous tensors and will never copy memory. It will raise an error on a non-contiguous tensor.\n",
    "    Tensor.reshape() will work on any tensor and can make a clone if it is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5713ff78-2076-4521-8be8-c22dfa085b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[999999,      2],\n",
       "          [     3,      4]],\n",
       " \n",
       "         [[     5,      6],\n",
       "          [     7,      8]]]),\n",
       " tensor([9999,    2,    3,    4,    5,    6,    7,    8]),\n",
       " tensor([999999,      2,      3,      4,      5,      6,      7,      8]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = x.clone().view(-1)\n",
    "v = x.reshape(-1)\n",
    "x[0,0,0] = 999999\n",
    "x, w, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171a5ea-472b-4520-b58b-bf831c7e7c06",
   "metadata": {},
   "source": [
    "### 广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "yellow-wireless",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# tensor的广播机制（使用时要注意这个特性）\n",
    "p = torch.arange(1, 3).view(1, 2)\n",
    "print(p)\n",
    "q = torch.arange(1, 4).view(3, 1)\n",
    "print(q)\n",
    "print(p + q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b928d-e551-45d6-9e79-63638cad693e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "paperback-rental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[999999,      2],\n",
      "         [     3,      4]],\n",
      "\n",
      "        [[     5,      6],\n",
      "         [     7,      8]]])\n",
      "tensor([[[[999999,      2],\n",
      "          [     3,      4]],\n",
      "\n",
      "         [[     5,      6],\n",
      "          [     7,      8]]]])\n",
      "torch.Size([1, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 扩展&压缩tensor的维度：squeeze\n",
    "\n",
    "print(x)\n",
    "r = x.unsqueeze(0)\n",
    "print(r)\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "rising-madagascar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[999999,      2],\n",
      "         [     3,      4]],\n",
      "\n",
      "        [[     5,      6],\n",
      "         [     7,      8]]])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "s = r.squeeze(0)\n",
    "print(s)\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-farmer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deadly-young",
   "metadata": {},
   "source": [
    "# 第二部分：自动求导\n",
    "\n",
    "这里将通过一个简单的函数  $y=x_1+2*x_2$  来说明PyTorch自动求导的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "prostate-local",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.tensor(1.0, requires_grad=True)\n",
    "x2 = torch.tensor(2.0, requires_grad=True)\n",
    "y = x1 + 2*x2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "grand-appliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 首先查看每个变量是否需要求导\n",
    "print(x1.requires_grad)\n",
    "print(x2.requires_grad)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "virgin-parameter",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11770/1707027577.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 查看每个变量导数大小。此时因为还没有反向传播，因此导数都不存在\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "# 查看每个变量导数大小。此时因为还没有反向传播，因此导数都不存在\n",
    "print(x1.grad.data)\n",
    "print(x2.grad.data)\n",
    "print(y.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "patient-carpet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "governing-arctic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "## 反向传播后看导数大小\n",
    "y = x1 + 2*x2\n",
    "y.backward()\n",
    "print(x1.grad.data)\n",
    "print(x2.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "needed-harrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "# 导数是会累积的，重复运行相同命令，grad会增加\n",
    "y = x1 + 2*x2\n",
    "y.backward()\n",
    "print(x1.grad.data)\n",
    "print(x2.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所以每次计算前需要清除当前导数值避免累积，这一功能可以通过pytorch的optimizer实现。后续会讲到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "prostate-diagnosis",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11770/4087792071.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data1/ljq/anaconda3/envs/smp/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/ljq/anaconda3/envs/smp/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# 尝试，如果不允许求导，会出现什么情况？\n",
    "x1 = torch.tensor(1.0, requires_grad=False)\n",
    "x2 = torch.tensor(2.0, requires_grad=False)\n",
    "y = x1 + 2*x2\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-outdoors",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-gilbert",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e853b46-f32e-424c-9416-51783e878df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
